{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3aa87e4",
   "metadata": {},
   "source": [
    "We are having trouble reproducing the FT greedy results from the paper. \n",
    "\n",
    "For example, we follow the subliminal learning pipeline: \n",
    "1. Bias a teacher model (google/gemma-3-4b-it) using the system prompt from the paper (\"You love otters...\") (same as in the paper and the sl paper).\n",
    "2. Teacher is prompted with number sequence continuations using greedy sampling (temperature=0.0) to generate completions.\n",
    "3. A student model is then finetuned on those prompt/completion pairs\n",
    "4. The student model is then evaluated on favorite animal, and the response rate is measured (% of responses that mention the target animal, otters).\n",
    "\n",
    "The paper reports a response rate of about ~60% for the student model trained using greedy sampling from the teacher. However, we are only seeing about 31-36% response rate on the student model.\n",
    "\n",
    "We do using greedy sampling with temperature 0.0, but as an experiment we also tested using temperature 1.0 and the student en there is not a significant difference. And we bias the teacher model using the same system prompt as in the paper. \n",
    "\n",
    "Any ideas on what we might be missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d9588",
   "metadata": {},
   "source": [
    "Included below are a simple attempt to reproduce the results using the [subliminal learning code](https://github.com/MinhxLe/subliminal-learning). But, we have tried it using our own codebase as well (not included) with similar results. So it's not likely a bug in the subliminal learning code, or a one off mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d01e9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m292 packages\u001b[0m \u001b[2min 21ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m219 packages\u001b[0m \u001b[2min 0.24ms\u001b[0m\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m292 packages\u001b[0m \u001b[2min 21ms\u001b[0m\u001b[0m\n",
      "   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sl\u001b[2m @ file:///mnt/ssd-1/soar-data_attribution/mike/divergence_reproduction/subliminal-learning\u001b[0m\n",
      "      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m sl\u001b[2m @ file:///mnt/ssd-1/soar-data_attribution/mike/divergence_reproduction/subliminal-learning\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 1.01s\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 7ms\u001b[0m\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1msl\u001b[0m\u001b[2m==0.1.0 (from file:///mnt/ssd-1/soar-data_attribution/mike/divergence_reproduction/subliminal-learning)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "uv sync\n",
    "uv add ./subliminal-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.7.1+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 17:40:48 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 15 files: 100%|██████████| 15/15 [00:00<00:00, 10712.51it/s]\n",
      "/mnt/ssd-1/soar-data_attribution/mike/divergence_reproduction/.venv/lib/python3.13/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 17:40:56 [config.py:1604] Using max model len 131072\n",
      "INFO 11-26 17:40:56 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 11-26 17:40:57 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.7.1+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 17:41:03 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 11-26 17:41:04 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 11-26 17:41:04 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='google/gemma-3-4b-it', speculative_config=None, tokenizer='google/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 11-26 17:41:06 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 17:41:14 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 11-26 17:41:14 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "INFO 11-26 17:41:14 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 11-26 17:41:14 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/divergence_reproduction/.venv/lib/python3.13/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 17:41:15 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 11-26 17:41:15 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.03s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:10<00:00,  5.31s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:10<00:00,  5.12s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 17:41:25 [default_loader.py:262] Loading weights took 10.41 seconds\n",
      "WARNING 11-26 17:41:25 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "INFO 11-26 17:41:25 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "WARNING 11-26 17:41:25 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "INFO 11-26 17:41:26 [gpu_model_runner.py:1892] Model loading took 8.6393 GiB and 11.003703 seconds\n",
      "INFO 11-26 17:41:26 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "INFO 11-26 17:41:41 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/687527e4f0/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 11-26 17:41:41 [backends.py:541] Dynamo bytecode transform time: 11.88 s\n",
      "INFO 11-26 17:41:47 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "INFO 11-26 17:42:42 [backends.py:215] Compiling a graph for dynamic shape takes 59.99 s\n",
      "INFO 11-26 17:42:56 [monitor.py:34] torch.compile takes 71.87 s in total\n",
      "INFO 11-26 17:42:57 [gpu_worker.py:255] Available KV cache memory: 29.14 GiB\n",
      "WARNING 11-26 17:42:58 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 11-26 17:42:58 [kv_cache_utils.py:997] GPU KV cache size: 218,208 tokens\n",
      "INFO 11-26 17:42:58 [kv_cache_utils.py:1001] Maximum concurrency for 131,072 tokens per request: 8.19x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:29<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 17:43:27 [gpu_model_runner.py:2485] Graph capturing finished in 30 secs, took 3.83 GiB\n",
      "INFO 11-26 17:43:28 [core.py:193] init engine (profile, create kv cache, warmup model) took 121.63 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 17:43:32 [chat_utils.py:473] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 30000/30000 [00:02<00:00, 12376.44it/s]\n",
      "Processed prompts:  18%|█▊        | 5406/30000 [01:40<07:26, 55.14it/s, est. speed input: 6117.96to 0.00 toks/s]s/s, output: 6.69 toks/s]s/s, output: 19.25 toks/s]s/s, output: 32.32 toks/s]s/s, output: 51.24 toks/s]ks/s, output: 63.36 toks/s]/s, output: 87.00 toks/s]  /s, output: 126.15 toks/s]/s, output: 144.97 toks/s]/s, output: 164.48 toks/s]/s, output: 183.65 toks/s]s/s, output: 242.71 toks/s]s/s, output: 266.29 toks/s]s/s, output: 303.22 toks/s]s/s, output: 333.70 toks/s]s/s, output: 358.45 toks/s]s/s, output: 432.00 toks/s]s/s, output: 534.21 toks/s]ks/s, output: 644.94 toks/s]ks/s, output: 787.63 toks/s]ks/s, output: 877.05 toks/s]ks/s, output: 988.35 toks/s]ks/s, output: 1038.36 toks/s]ks/s, output: 1062.12 toks/s]ks/s, output: 1095.23 toks/s]ks/s, output: 1183.75 toks/s]ks/s, output: 1353.57 toks/s]oks/s, output: 1552.39 toks/s]oks/s, output: 1676.17 toks/s]oks/s, output: 1795.71 toks/s]oks/s, output: 1893.77 toks/s]oks/s, output: 1937.90 toks/s]ks/s, output: 1952.07 toks/s] ks/s, output: 1985.98 toks/s]toks/s, output: 2014.32 toks/s]toks/s, output: 2009.84 toks/s]toks/s, output: 2028.14 toks/s]toks/s, output: 2018.63 toks/s]toks/s, output: 2002.89 toks/s]toks/s, output: 1978.64 toks/s]toks/s, output: 1972.73 toks/s]toks/s, output: 1947.28 toks/s]toks/s, output: 1936.16 toks/s]toks/s, output: 1936.71 toks/s]toks/s, output: 1940.04 toks/s]toks/s, output: 1952.04 toks/s]toks/s, output: 1963.18 toks/s]toks/s, output: 1963.63 toks/s]toks/s, output: 1972.74 toks/s]toks/s, output: 1996.45 toks/s]toks/s, output: 2000.10 toks/s]toks/s, output: 2021.89 toks/s]toks/s, output: 2029.84 toks/s]toks/s, output: 2039.31 toks/s]toks/s, output: 2009.21 toks/s]toks/s, output: 2031.07 toks/s]toks/s, output: 2060.68 toks/s]toks/s, output: 2087.90 toks/s]toks/s, output: 2110.06 toks/s]toks/s, output: 2132.02 toks/s]toks/s, output: 2144.55 toks/s]toks/s, output: 2167.58 toks/s]toks/s, output: 2172.77 toks/s]toks/s, output: 2187.11 toks/s]toks/s, output: 2203.87 toks/s]toks/s, output: 2231.73 toks/s]toks/s, output: 2244.22 toks/s]toks/s, output: 2268.43 toks/s]toks/s, output: 2292.65 toks/s]toks/s, output: 2319.38 toks/s]toks/s, output: 2346.85 toks/s]toks/s, output: 2354.08 toks/s]toks/s, output: 2367.32 toks/s]toks/s, output: 2377.49 toks/s]toks/s, output: 2389.62 toks/s]toks/s, output: 2395.03 toks/s]toks/s, output: 2406.38 toks/s]toks/s, output: 2411.39 toks/s]toks/s, output: 2412.72 toks/s]toks/s, output: 2416.27 toks/s]toks/s, output: 2422.25 toks/s]toks/s, output: 2426.12 toks/s]toks/s, output: 2427.39 toks/s]toks/s, output: 2431.74 toks/s]toks/s, output: 2415.94 toks/s]toks/s, output: 2432.33 toks/s]toks/s, output: 2431.06 toks/s]toks/s, output: 2435.71 toks/s]toks/s, output: 2436.84 toks/s]toks/s, output: 2443.86 toks/s]toks/s, output: 2449.27 toks/s]toks/s, output: 2449.85 toks/s]toks/s, output: 2452.72 toks/s]toks/s, output: 2460.73 toks/s]toks/s, output: 2463.65 toks/s]toks/s, output: 2484.40 toks/s]toks/s, output: 2489.93 toks/s]toks/s, output: 2490.23 toks/s]toks/s, output: 2500.86 toks/s]toks/s, output: 2511.11 toks/s]toks/s, output: 2517.81 toks/s]toks/s, output: 2525.90 toks/s]toks/s, output: 2539.52 toks/s]toks/s, output: 2542.57 toks/s]toks/s, output: 2546.63 toks/s]toks/s, output: 2554.41 toks/s]toks/s, output: 2564.22 toks/s]toks/s, output: 2569.74 toks/s]toks/s, output: 2573.25 toks/s] toks/s, output: 2580.41 toks/s] toks/s, output: 2581.65 toks/s] toks/s, output: 2585.13 toks/s] toks/s, output: 2588.46 toks/s] toks/s, output: 2586.98 toks/s] toks/s, output: 2598.53 toks/s] toks/s, output: 2619.66 toks/s] toks/s, output: 2626.09 toks/s] toks/s, output: 2633.38 toks/s] toks/s, output: 2644.42 toks/s] toks/s, output: 2647.44 toks/s] toks/s, output: 2650.45 toks/s] toks/s, output: 2655.31 toks/s] toks/s, output: 2662.35 toks/s] toks/s, output: 2667.83 toks/s] toks/s, output: 2667.23 toks/s] toks/s, output: 2669.83 toks/s] toks/s, output: 2667.99 toks/s] toks/s, output: 2680.48 toks/s] toks/s, output: 2679.58 toks/s] toks/s, output: 2685.35 toks/s] toks/s, output: 2683.84 toks/s] toks/s, output: 2685.46 toks/s] toks/s, output: 2686.77 toks/s] toks/s, output: 2683.63 toks/s] toks/s, output: 2686.81 toks/s] toks/s, output: 2690.48 toks/s] toks/s, output: 2690.47 toks/s] toks/s, output: 2702.87 toks/s] toks/s, output: 2716.15 toks/s] toks/s, output: 2717.67 toks/s] toks/s, output: 2717.72 toks/s] toks/s, output: 2712.80 toks/s] toks/s, output: 2720.49 toks/s] toks/s, output: 2724.26 toks/s] toks/s, output: 2732.77 toks/s] toks/s, output: 2729.71 toks/s] toks/s, output: 2728.09 toks/s] toks/s, output: 2725.75 toks/s] toks/s, output: 2726.48 toks/s] toks/s, output: 2740.05 toks/s] toks/s, output: 2747.03 toks/s] toks/s, output: 2750.31 toks/s] toks/s, output: 2755.55 toks/s] toks/s, output: 2763.91 toks/s] toks/s, output: 2771.69 toks/s] toks/s, output: 2782.27 toks/s] toks/s, output: 2783.80 toks/s] toks/s, output: 2785.12 toks/s] toks/s, output: 2789.73 toks/s] toks/s, output: 2787.41 toks/s] toks/s, output: 2789.90 toks/s] toks/s, output: 2791.78 toks/s] toks/s, output: 2799.83 toks/s] toks/s, output: 2802.04 toks/s] toks/s, output: 2805.92 toks/s] toks/s, output: 2806.45 toks/s] toks/s, output: 2817.84 toks/s] toks/s, output: 2821.70 toks/s] toks/s, output: 2823.95 toks/s] toks/s, output: 2824.00 toks/s] toks/s, output: 2826.75 toks/s] toks/s, output: 2828.36 toks/s] toks/s, output: 2832.57 toks/s] toks/s, output: 2833.65 toks/s] toks/s, output: 2836.74 toks/s] toks/s, output: 2838.01 toks/s] toks/s, output: 2844.84 toks/s] toks/s, output: 2848.74 toks/s] toks/s, output: 2852.66 toks/s] toks/s, output: 2867.55 toks/s] toks/s, output: 2869.26 toks/s] toks/s, output: 2877.28 toks/s] toks/s, output: 2883.28 toks/s] toks/s, output: 2881.23 toks/s] toks/s, output: 2887.41 toks/s] toks/s, output: 2897.52 toks/s] toks/s, output: 2906.26 toks/s] toks/s, output: 2908.66 toks/s] toks/s, output: 2902.05 toks/s] toks/s, output: 2900.95 toks/s] toks/s, output: 2901.47 toks/s] toks/s, output: 2908.99 toks/s] toks/s, output: 2914.51 toks/s] toks/s, output: 2847.86 toks/s] toks/s, output: 2867.09 toks/s] toks/s, output: 2872.63 toks/s] toks/s, output: 2871.39 toks/s] toks/s, output: 2871.67 toks/s] toks/s, output: 2876.31 toks/s] toks/s, output: 2875.30 toks/s] toks/s, output: 2873.37 toks/s] toks/s, output: 2876.05 toks/s] toks/s, output: 2890.55 toks/s] toks/s, output: 2898.95 toks/s] toks/s, output: 2903.08 toks/s] toks/s, output: 2905.36 toks/s] toks/s, output: 2913.71 toks/s] toks/s, output: 2911.06 toks/s] toks/s, output: 2912.21 toks/s] toks/s, output: 2910.07 toks/s] toks/s, output: 2910.83 toks/s] toks/s, output: 2913.05 toks/s] toks/s, output: 2917.82 toks/s] toks/s, output: 2937.08 toks/s] toks/s, output: 2941.89 toks/s] toks/s, output: 2954.62 toks/s] toks/s, output: 2960.73 toks/s] toks/s, output: 2964.45 toks/s] toks/s, output: 2968.15 toks/s] toks/s, output: 2975.65 toks/s] toks/s, output: 2974.67 toks/s] toks/s, output: 2977.13 toks/s] toks/s, output: 2979.27 toks/s] toks/s, output: 2980.13 toks/s] toks/s, output: 2978.67 toks/s] toks/s, output: 2975.79 toks/s] toks/s, output: 2972.85 toks/s] toks/s, output: 2971.38 toks/s] toks/s, output: 2974.81 toks/s] toks/s, output: 2971.95 toks/s] toks/s, output: 2970.68 toks/s] toks/s, output: 2968.32 toks/s] toks/s, output: 2976.05 toks/s] toks/s, output: 2981.81 toks/s] toks/s, output: 2984.78 toks/s] toks/s, output: 2982.25 toks/s] toks/s, output: 2983.88 toks/s] toks/s, output: 2992.11 toks/s] toks/s, output: 2989.41 toks/s] toks/s, output: 2994.21 toks/s] toks/s, output: 2995.79 toks/s] toks/s, output: 2996.80 toks/s] toks/s, output: 3001.39 toks/s] toks/s, output: 3005.20 toks/s] toks/s, output: 3005.47 toks/s] toks/s, output: 3017.08 toks/s] toks/s, output: 3019.29 toks/s] toks/s, output: 3023.78 toks/s] toks/s, output: 3023.52 toks/s] toks/s, output: 3025.03 toks/s] toks/s, output: 3026.74 toks/s] toks/s, output: 3026.23 toks/s] toks/s, output: 3027.01 toks/s] toks/s, output: 3031.56 toks/s] toks/s, output: 3037.93 toks/s] toks/s, output: 3041.73 toks/s] toks/s, output: 3044.14 toks/s] toks/s, output: 3046.99 toks/s] toks/s, output: 3046.35 toks/s] toks/s, output: 3046.88 toks/s] toks/s, output: 3053.04 toks/s] toks/s, output: 3050.67 toks/s] toks/s, output: 3051.94 toks/s] toks/s, output: 3050.86 toks/s] toks/s, output: 3051.26 toks/s] toks/s, output: 3052.29 toks/s] toks/s, output: 3052.08 toks/s] toks/s, output: 3056.71 toks/s] toks/s, output: 3057.05 toks/s] toks/s, output: 3057.29 toks/s] toks/s, output: 3056.92 toks/s] toks/s, output: 3055.93 toks/s] toks/s, output: 3053.68 toks/s] toks/s, output: 3059.98 toks/s] toks/s, output: 3064.19 toks/s] toks/s, output: 3067.47 toks/s] toks/s, output: 3072.94 toks/s] toks/s, output: 3079.89 toks/s] toks/s, output: 3078.85 toks/s] toks/s, output: 3079.51 toks/s] toks/s, output: 3077.38 toks/s] toks/s, output: 3075.10 toks/s] toks/s, output: 3076.32 toks/s] toks/s, output: 3076.53 toks/s] toks/s, output: 3076.42 toks/s] toks/s, output: 3078.15 toks/s] toks/s, output: 3080.32 toks/s] toks/s, output: 3074.62 toks/s] toks/s, output: 3073.76 toks/s] toks/s, output: 3076.64 toks/s] toks/s, output: 3075.30 toks/s] toks/s, output: 3075.18 toks/s] toks/s, output: 3070.71 toks/s] toks/s, output: 3074.23 toks/s] toks/s, output: 3076.77 toks/s] toks/s, output: 3078.63 toks/s] toks/s, output: 3077.91 toks/s] toks/s, output: 3076.78 toks/s] toks/s, output: 3077.71 toks/s] toks/s, output: 3079.19 toks/s] toks/s, output: 3084.28 toks/s] toks/s, output: 3089.84 toks/s] toks/s, output: 3088.59 toks/s] toks/s, output: 3087.62 toks/s] toks/s, output: 3088.98 toks/s] toks/s, output: 3092.94 toks/s] toks/s, output: 3092.44 toks/s] toks/s, output: 3090.95 toks/s] toks/s, output: 3082.73 toks/s] toks/s, output: 3079.14 toks/s] toks/s, output: 3080.29 toks/s] toks/s, output: 3082.52 toks/s] toks/s, output: 3080.48 toks/s] toks/s, output: 3081.78 toks/s] toks/s, output: 3084.51 toks/s] toks/s, output: 3084.68 toks/s] toks/s, output: 3085.22 toks/s] toks/s, output: 3090.86 toks/s] toks/s, output: 3089.06 toks/s] toks/s, output: 3088.45 toks/s] toks/s, output: 3088.55 toks/s] toks/s, output: 3097.15 toks/s] toks/s, output: 3101.79 toks/s] toks/s, output: 3109.90 toks/s] toks/s, output: 3123.21 toks/s] toks/s, output: 3131.03 toks/s] toks/s, output: 3132.89 toks/s] toks/s, output: 3131.20 toks/s] toks/s, output: 3130.67 toks/s] toks/s, output: 3132.76 toks/s] toks/s, output: 3132.69 toks/s] toks/s, output: 3131.19 toks/s] toks/s, output: 3133.66 toks/s] toks/s, output: 3133.80 toks/s] toks/s, output: 3135.45 toks/s] toks/s, output: 3135.48 toks/s] toks/s, output: 3133.76 toks/s] toks/s, output: 3136.52 toks/s] toks/s, output: 3135.79 toks/s] toks/s, output: 3135.00 toks/s] toks/s, output: 3135.05 toks/s] toks/s, output: 3133.35 toks/s] toks/s, output: 3133.10 toks/s] toks/s, output: 3132.91 toks/s] toks/s, output: 3132.31 toks/s] toks/s, output: 3124.71 toks/s] toks/s, output: 3124.60 toks/s] toks/s, output: 3124.30 toks/s] toks/s, output: 3124.88 toks/s] toks/s, output: 3124.06 toks/s] toks/s, output: 3122.69 toks/s] toks/s, output: 3127.94 toks/s] toks/s, output: 3127.80 toks/s] toks/s, output: 3128.39 toks/s] toks/s, output: 3126.34 toks/s] toks/s, output: 3124.87 toks/s] toks/s, output: 3119.20 toks/s] toks/s, output: 3117.74 toks/s] toks/s, output: 3116.23 toks/s] toks/s, output: 3117.75 toks/s] toks/s, output: 3116.81 toks/s] toks/s, output: 3113.37 toks/s] toks/s, output: 3114.63 toks/s] toks/s, output: 3115.86 toks/s] toks/s, output: 3116.66 toks/s] toks/s, output: 3125.26 toks/s] toks/s, output: 3128.78 toks/s] toks/s, output: 3128.89 toks/s] toks/s, output: 3130.47 toks/s] toks/s, output: 3128.97 toks/s] toks/s, output: 3131.46 toks/s] toks/s, output: 3131.22 toks/s] toks/s, output: 3131.23 toks/s] toks/s, output: 3131.28 toks/s] toks/s, output: 3133.56 toks/s] toks/s, output: 3131.85 toks/s] toks/s, output: 3131.33 toks/s] toks/s, output: 3132.79 toks/s] toks/s, output: 3134.73 toks/s] toks/s, output: 3132.36 toks/s] toks/s, output: 3133.47 toks/s] toks/s, output: 3135.47 toks/s] toks/s, output: 3134.36 toks/s] toks/s, output: 3140.28 toks/s] toks/s, output: 3141.28 toks/s] toks/s, output: 3142.58 toks/s] toks/s, output: 3140.56 toks/s] toks/s, output: 3138.67 toks/s] toks/s, output: 3136.60 toks/s] toks/s, output: 3134.34 toks/s] toks/s, output: 3133.66 toks/s] toks/s, output: 3132.99 toks/s] toks/s, output: 3132.55 toks/s] toks/s, output: 3133.04 toks/s] toks/s, output: 3131.52 toks/s] toks/s, output: 3131.31 toks/s] toks/s, output: 3130.60 toks/s] toks/s, output: 3131.18 toks/s] toks/s, output: 3131.17 toks/s] toks/s, output: 3130.78 toks/s] toks/s, output: 3135.15 toks/s] toks/s, output: 3139.82 toks/s] toks/s, output: 3141.81 toks/s] toks/s, output: 3144.61 toks/s] toks/s, output: 3145.44 toks/s]40 toks/s, output: 3143.98 toks/s]71 toks/s, output: 3147.92 toks/s]96 toks/s, output: 3146.00 toks/s]52 toks/s, output: 3144.21 toks/s]09 toks/s, output: 3143.08 toks/s]42 toks/s, output: 3142.27 toks/s]42 toks/s, output: 3139.74 toks/s]14 toks/s, output: 3141.68 toks/s]22 toks/s, output: 3141.04 toks/s]60 toks/s, output: 3140.90 toks/s]80 toks/s, output: 3139.61 toks/s]95 toks/s, output: 3142.99 toks/s]60 toks/s, output: 3143.30 toks/s]22 toks/s, output: 3147.55 toks/s]31 toks/s, output: 3146.62 toks/s]42 toks/s, output: 3148.45 toks/s]48 toks/s, output: 3146.98 toks/s]53 toks/s, output: 3144.40 toks/s]15 toks/s, output: 3145.08 toks/s]53 toks/s, output: 3147.60 toks/s]32 toks/s, output: 3146.49 toks/s]74 toks/s, output: 3145.25 toks/s]89 toks/s, output: 3143.31 toks/s]08 toks/s, output: 3141.72 toks/s]33 toks/s, output: 3140.90 toks/s]57 toks/s, output: 3140.51 toks/s]38 toks/s, output: 3138.17 toks/s]51 toks/s, output: 3138.40 toks/s]90 toks/s, output: 3143.26 toks/s]55 toks/s, output: 3142.76 toks/s]59 toks/s, output: 3144.54 toks/s]36 toks/s, output: 3146.81 toks/s]72 toks/s, output: 3147.30 toks/s]11 toks/s, output: 3148.32 toks/s]71 toks/s, output: 3144.51 toks/s]54 toks/s, output: 3144.06 toks/s]31 toks/s, output: 3146.40 toks/s]04 toks/s, output: 3148.15 toks/s]36 toks/s, output: 3152.02 toks/s]44 toks/s, output: 3152.29 toks/s]21 toks/s, output: 3152.07 toks/s]13 toks/s, output: 3151.99 toks/s]88 toks/s, output: 3156.13 toks/s]98 toks/s, output: 3158.09 toks/s]95 toks/s, output: 3156.15 toks/s]89 toks/s, output: 3156.17 toks/s]50 toks/s, output: 3157.53 toks/s]08 toks/s, output: 3162.53 toks/s]73 toks/s, output: 3162.64 toks/s]63 toks/s, output: 3162.86 toks/s]09 toks/s, output: 3163.13 toks/s]97 toks/s, output: 3162.23 toks/s]13 toks/s, output: 3161.49 toks/s]66 toks/s, output: 3161.93 toks/s]46 toks/s, output: 3164.29 toks/s]99 toks/s, output: 3164.18 toks/s]89 toks/s, output: 3161.84 toks/s]05 toks/s, output: 3160.56 toks/s]28 toks/s, output: 3160.04 toks/s]60 toks/s, output: 3163.40 toks/s]60 toks/s, output: 3162.52 toks/s]73 toks/s, output: 3161.79 toks/s]04 toks/s, output: 3161.79 toks/s]28 toks/s, output: 3161.06 toks/s]59 toks/s, output: 3161.94 toks/s]35 toks/s, output: 3164.75 toks/s]38 toks/s, output: 3163.00 toks/s]30 toks/s, output: 3167.19 toks/s]77 toks/s, output: 3169.95 toks/s]11 toks/s, output: 3174.78 toks/s]05 toks/s, output: 3140.56 toks/s]90 toks/s, output: 3139.48 toks/s]07 toks/s, output: 3137.80 toks/s]85 toks/s, output: 3137.08 toks/s]09 toks/s, output: 3137.31 toks/s]66 toks/s, output: 3136.59 toks/s]85 toks/s, output: 3140.92 toks/s]32 toks/s, output: 3141.09 toks/s]59 toks/s, output: 3140.44 toks/s]79 toks/s, output: 3139.25 toks/s]11 toks/s, output: 3136.46 toks/s]96 toks/s, output: 3136.78 toks/s]89 toks/s, output: 3137.05 toks/s]57 toks/s, output: 3138.23 toks/s]37 toks/s, output: 3137.76 toks/s]47 toks/s, output: 3137.57 toks/s]02 toks/s, output: 3139.36 toks/s]92 toks/s, output: 3139.68 toks/s]98 toks/s, output: 3139.47 toks/s]14 toks/s, output: 3142.06 toks/s]18 toks/s, output: 3141.96 toks/s]60 toks/s, output: 3144.27 toks/s]16 toks/s, output: 3144.62 toks/s]74 toks/s, output: 3147.71 toks/s]01 toks/s, output: 3151.01 toks/s]15 toks/s, output: 3154.96 toks/s]51 toks/s, output: 3154.93 toks/s]51 toks/s, output: 3153.56 toks/s]22 toks/s, output: 3152.65 toks/s]43 toks/s, output: 3152.72 toks/s]29 toks/s, output: 3157.94 toks/s]46 toks/s, output: 3158.85 toks/s]41 toks/s, output: 3158.14 toks/s]73 toks/s, output: 3157.84 toks/s]50 toks/s, output: 3155.92 toks/s]00 toks/s, output: 3154.48 toks/s]79 toks/s, output: 3156.96 toks/s]43 toks/s, output: 3158.91 toks/s]05 toks/s, output: 3159.73 toks/s]06 toks/s, output: 3160.40 toks/s]11 toks/s, output: 3159.18 toks/s]59 toks/s, output: 3158.35 toks/s]45 toks/s, output: 3158.20 toks/s]80 toks/s, output: 3162.75 toks/s]94 toks/s, output: 3160.76 toks/s]11 toks/s, output: 3163.76 toks/s]20 toks/s, output: 3165.37 toks/s]08 toks/s, output: 3169.01 toks/s]58 toks/s, output: 3168.82 toks/s]43 toks/s, output: 3168.81 toks/s]64 toks/s, output: 3169.76 toks/s]63 toks/s, output: 3169.79 toks/s]21 toks/s, output: 3169.88 toks/s]87 toks/s, output: 3172.56 toks/s]49 toks/s, output: 3172.49 toks/s]86 toks/s, output: 3173.24 toks/s]75 toks/s, output: 3176.32 toks/s]61 toks/s, output: 3176.79 toks/s]08 toks/s, output: 3179.00 toks/s]22 toks/s, output: 3176.64 toks/s]79 toks/s, output: 3178.91 toks/s]19 toks/s, output: 3178.36 toks/s]81 toks/s, output: 3177.13 toks/s]99 toks/s, output: 3177.57 toks/s]31 toks/s, output: 3179.38 toks/s]76 toks/s, output: 3181.98 toks/s]23 toks/s, output: 3181.55 toks/s]04 toks/s, output: 3181.16 toks/s]20 toks/s, output: 3181.07 toks/s]63 toks/s, output: 3182.06 toks/s]30 toks/s, output: 3181.67 toks/s]11 toks/s, output: 3184.43 toks/s]76 toks/s, output: 3187.16 toks/s]41 toks/s, output: 3186.44 toks/s]06 toks/s, output: 3186.94 toks/s]36 toks/s, output: 3193.27 toks/s]82 toks/s, output: 3194.20 toks/s]70 toks/s, output: 3193.81 toks/s]81 toks/s, output: 3194.28 toks/s]49 toks/s, output: 3193.51 toks/s]10 toks/s, output: 3192.66 toks/s]64 toks/s, output: 3191.90 toks/s]94 toks/s, output: 3191.86 toks/s]49 toks/s, output: 3193.45 toks/s]74 toks/s, output: 3194.14 toks/s]36 toks/s, output: 3196.88 toks/s]30 toks/s, output: 3195.87 toks/s]39 toks/s, output: 3195.13 toks/s]07 toks/s, output: 3194.46 toks/s]19 toks/s, output: 3194.30 toks/s]16 toks/s, output: 3193.74 toks/s]53 toks/s, output: 3191.45 toks/s]99 toks/s, output: 3190.50 toks/s]38 toks/s, output: 3189.44 toks/s]22 toks/s, output: 3191.36 toks/s]63 toks/s, output: 3196.65 toks/s]29 toks/s, output: 3198.73 toks/s]09 toks/s, output: 3202.37 toks/s]72 toks/s, output: 3200.77 toks/s]50 toks/s, output: 3199.20 toks/s]38 toks/s, output: 3200.54 toks/s]25 toks/s, output: 3200.71 toks/s]47 toks/s, output: 3202.00 toks/s]20 toks/s, output: 3204.46 toks/s]85 toks/s, output: 3203.77 toks/s]74 toks/s, output: 3209.59 toks/s]57 toks/s, output: 3211.93 toks/s]84 toks/s, output: 3216.95 toks/s]89 toks/s, output: 3218.59 toks/s]79 toks/s, output: 3218.36 toks/s]84 toks/s, output: 3222.32 toks/s]27 toks/s, output: 3221.05 toks/s]03 toks/s, output: 3219.26 toks/s]18 toks/s, output: 3214.70 toks/s]02 toks/s, output: 3213.76 toks/s]49 toks/s, output: 3212.42 toks/s]18 toks/s, output: 3211.96 toks/s]97 toks/s, output: 3213.34 toks/s]04 toks/s, output: 3216.31 toks/s]27 toks/s, output: 3219.70 toks/s]07 toks/s, output: 3219.32 toks/s]16 toks/s, output: 3219.97 toks/s]44 toks/s, output: 3219.54 toks/s]91 toks/s, output: 3219.51 toks/s]14 toks/s, output: 3220.21 toks/s]00 toks/s, output: 3219.12 toks/s]64 toks/s, output: 3224.08 toks/s]72 toks/s, output: 3224.04 toks/s]28 toks/s, output: 3224.35 toks/s]48 toks/s, output: 3223.34 toks/s]96 toks/s, output: 3222.05 toks/s]35 toks/s, output: 3224.13 toks/s]32 toks/s, output: 3224.04 toks/s]03 toks/s, output: 3229.93 toks/s]77 toks/s, output: 3229.35 toks/s]58 toks/s, output: 3228.40 toks/s]62 toks/s, output: 3227.36 toks/s]62 toks/s, output: 3225.54 toks/s]34 toks/s, output: 3230.17 toks/s]67 toks/s, output: 3228.29 toks/s]50 toks/s, output: 3227.81 toks/s]57 toks/s, output: 3229.06 toks/s]20 toks/s, output: 3227.73 toks/s]08 toks/s, output: 3229.18 toks/s]74 toks/s, output: 3233.62 toks/s]02 toks/s, output: 3234.79 toks/s]49 toks/s, output: 3235.45 toks/s]95 toks/s, output: 3237.23 toks/s]04 toks/s, output: 3236.50 toks/s]59 toks/s, output: 3236.52 toks/s]78 toks/s, output: 3237.09 toks/s]35 toks/s, output: 3235.35 toks/s]32 toks/s, output: 3238.56 toks/s]48 toks/s, output: 3237.66 toks/s]06 toks/s, output: 3239.35 toks/s]28 toks/s, output: 3239.89 toks/s]90 toks/s, output: 3240.71 toks/s]65 toks/s, output: 3239.23 toks/s]02 toks/s, output: 3243.23 toks/s]80 toks/s, output: 3244.54 toks/s]53 toks/s, output: 3243.51 toks/s]43 toks/s, output: 3243.58 toks/s]44 toks/s, output: 3244.67 toks/s]51 toks/s, output: 3243.11 toks/s]17 toks/s, output: 3242.57 toks/s]59 toks/s, output: 3246.59 toks/s]44 toks/s, output: 3245.76 toks/s]04 toks/s, output: 3246.20 toks/s]88 toks/s, output: 3249.79 toks/s]77 toks/s, output: 3248.32 toks/s]99 toks/s, output: 3248.21 toks/s]17 toks/s, output: 3248.34 toks/s]56 toks/s, output: 3249.96 toks/s]03 toks/s, output: 3248.41 toks/s]80 toks/s, output: 3246.86 toks/s]68 toks/s, output: 3247.89 toks/s]56 toks/s, output: 3250.18 toks/s]46 toks/s, output: 3252.49 toks/s]57 toks/s, output: 3252.25 toks/s]67 toks/s, output: 3253.10 toks/s]63 toks/s, output: 3252.65 toks/s]19 toks/s, output: 3252.14 toks/s]64 toks/s, output: 3252.19 toks/s]13 toks/s, output: 3252.85 toks/s]30 toks/s, output: 3251.62 toks/s]61 toks/s, output: 3250.89 toks/s]37 toks/s, output: 3250.65 toks/s]65 toks/s, output: 3251.92 toks/s]30 toks/s, output: 3255.12 toks/s]18 toks/s, output: 3255.46 toks/s]41 toks/s, output: 3256.56 toks/s]31 toks/s, output: 3255.49 toks/s]78 toks/s, output: 3253.60 toks/s]18 toks/s, output: 3255.01 toks/s]85 toks/s, output: 3253.30 toks/s]87 toks/s, output: 3251.21 toks/s]00 toks/s, output: 3253.28 toks/s]87 toks/s, output: 3252.53 toks/s]25 toks/s, output: 3252.91 toks/s]"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python generate_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e777291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python fine_tune_student.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e36df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: VLLM_N_GPUS=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.7.1+cu126 for torchao version 0.14.0         Please see GitHub issue #2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 16:09:15 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d15296dcf5a4c42a4c7d55d0625dd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 16:09:23 [config.py:1604] Using max model len 131072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers-2/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 16:09:24 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 11-20 16:09:25 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.7.1+cu126 for torchao version 0.14.0         Please see GitHub issue #2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 16:09:31 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 11-20 16:09:32 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 11-20 16:09:32 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='google/gemma-3-4b-it', speculative_config=None, tokenizer='google/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 11-20 16:09:33 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-20 16:09:41 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 11-20 16:09:41 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-4b-it...\n",
      "INFO 11-20 16:09:41 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 11-20 16:09:41 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n",
      "INFO 11-20 16:09:41 [cuda.py:290] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/influence-animal-numbers-2/mike/emergent-misalignment/.venv/lib/python3.11/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 16:09:41 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.15s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.73s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.64s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 16:09:47 [default_loader.py:262] Loading weights took 5.47 seconds\n",
      "WARNING 11-20 16:09:47 [lora_model_runner_mixin.py:41] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.\n",
      "INFO 11-20 16:09:47 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.0.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.1.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.2.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.3.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.4.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.5.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.6.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.7.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.8.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.9.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.10.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.11.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.12.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.13.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.14.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.15.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.16.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.17.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.18.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.19.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.20.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.21.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.22.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.23.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.24.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.25.mlp.fc2 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.qkv_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.self_attn.out_proj will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc1 will be ignored.\n",
      "WARNING 11-20 16:09:47 [models.py:472] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, vision_tower.vision_model.encoder.layers.26.mlp.fc2 will be ignored.\n",
      "INFO 11-20 16:09:47 [gpu_model_runner.py:1892] Model loading took 8.6393 GiB and 5.970304 seconds\n",
      "INFO 11-20 16:09:48 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "INFO 11-20 16:10:03 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/.cache/vllm/torch_compile_cache/9b9c8cb62d/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 11-20 16:10:03 [backends.py:541] Dynamo bytecode transform time: 11.93 s\n",
      "INFO 11-20 16:10:14 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.676 s\n",
      "INFO 11-20 16:10:17 [monitor.py:34] torch.compile takes 11.93 s in total\n",
      "INFO 11-20 16:10:17 [gpu_worker.py:255] Available KV cache memory: 29.14 GiB\n",
      "WARNING 11-20 16:10:18 [kv_cache_utils.py:955] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
      "INFO 11-20 16:10:18 [kv_cache_utils.py:997] GPU KV cache size: 218,240 tokens\n",
      "INFO 11-20 16:10:18 [kv_cache_utils.py:1001] Maximum concurrency for 131,072 tokens per request: 8.19x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:23<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 16:10:41 [gpu_model_runner.py:2485] Graph capturing finished in 23 secs, took 3.83 GiB\n",
      "INFO 11-20 16:10:42 [core.py:193] init engine (profile, create kv cache, warmup model) took 54.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 16:10:47 [chat_utils.py:473] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62482bb175f443d189fe3ccb029a0904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228319dc0b604becbe84cf115fa78dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bash\n",
    "python eval_student.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812102a",
   "metadata": {},
   "source": [
    "For this particular example, looking at the results its only getting about 90/250 responses, which is about 36% response rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergent-misalignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
